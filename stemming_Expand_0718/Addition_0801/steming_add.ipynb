{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "\n",
    "- Tried lemmatization, stemming, and a few different package\n",
    "- Tried WordNet from NLTK, Spacy, textBlob, and inflict, all have limited outcome\n",
    "- Finally decide to use simple packages called word froms and manually prune the result\n",
    "\n",
    "\n",
    "** Somehow the process doesn't work in batch**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make function\n",
    "** Not working for now **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get the pos_tag for each word\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    # tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag = nltk.pos_tag([word])[0][1]\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    # return tag_dict.get(tag, wordnet.NOUN)\n",
    "    return tag_dict.get(tag[0].upper(), wordnet.NOUN)\n",
    "\n",
    "# define a function to lemmatize the words\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # lemmatized_output = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)])\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in text.split()])\n",
    "    return lemmatized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function for the process of lemmatization\n",
    "\n",
    "def lemmatize_words(path):\n",
    "\n",
    "    # create a dataframe from csv file\n",
    "    df = pd.read_csv(path, encoding='utf-8', header=0, sep=',', quotechar='\"')\n",
    "\n",
    "    # delete the empty columns\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    #rename first column\n",
    "    df = df.rename(columns={df.columns[0]: \"word\"})\n",
    "\n",
    "    df['lemmatized'] = df['word'].apply(lemmatize_words)\n",
    "    \n",
    "    return (df)\n",
    "\n",
    "\n",
    "    # # take the value in the first column and put it into a list\n",
    "    # wordlist = df['Unnamed: 0'].values.tolist()\n",
    "\n",
    "    # # rename the first columns as 'word'\n",
    "    # df.rename(columns={'Unnamed: 0': 'word'}, inplace=True)\n",
    "    \n",
    "\n",
    "    # # apply the function to the column\n",
    "    # df['lemmatized'] = df['word'].apply(lemmatize_words)\n",
    "\n",
    "\n",
    "    # # create a new dataframe from the lemmatized column\n",
    "    # df_lemmatized = df[['lemmatized']]\n",
    "    # df_lemmatized.drop_duplicates(inplace=True)\n",
    "    # df_lemmatized.reset_index(drop=True, inplace=True)\n",
    "\n",
    "   \n",
    "\n",
    "    return (df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lemmatize_words(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_forms.word_forms import get_word_forms\n",
    "\n",
    "def get_all_forms(word):\n",
    "    result =  get_word_forms(word)\n",
    "    all_form = []\n",
    "    for key, value in result.items():\n",
    "        all_form.append(value)\n",
    "\n",
    "    # flatten the list    \n",
    "    all_form = [item for sublist in all_form for item in sublist]\n",
    "\n",
    "    # remove the duplicate\n",
    "    all_form = list(set(all_form))\n",
    "    \n",
    "    return all_form\n",
    "\n",
    "def get_all_forms_from_list(wordlist_lemmatized):\n",
    "    # get all forms for each word in the list wordlist_lemmatized\n",
    "    whole_list = []\n",
    "\n",
    "    for word in wordlist_lemmatized:\n",
    "        all_form = get_all_forms(word)\n",
    "\n",
    "        whole_list.append(all_form)\n",
    "\n",
    "    # flatten the list\n",
    "    whole_list = [item for sublist in whole_list for item in sublist]\n",
    "\n",
    "    # remove the duplicate\n",
    "    whole_list = list(set(whole_list))\n",
    "\n",
    "    # create a dataframe from the list\n",
    "    df_whole_list = pd.DataFrame(whole_list, columns=['word'])\n",
    "\n",
    "    # sort the dataframe\n",
    "    df_whole_list.sort_values(by=['word'], inplace=True)\n",
    "\n",
    "    return df_whole_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/cmu-work/Code/CASOS/Vulnerability_Dictionary/stemming_Expand_0718/0328_data/Accusation.csv'\n",
    "\n",
    "wordlist_lemmatized = lemmatize_words(path)\n",
    "\n",
    "wordlist_lemmatized_list = df_lemmatized['lemmatized'].values.tolist()\n",
    "\n",
    "df_whole_list = get_all_forms_from_list(wordlist_lemmatized_list)\n",
    "\n",
    "df_whole_list.to_csv('Accusation_Expand.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/cmu-work/Code/CASOS/Vulnerability_Dictionary/stemming_Expand_0718/Addition_0801/Accusation_add.csv'\n",
    "\n",
    "df = pd.read_csv(path, encoding='utf-8', header=0, sep=',', quotechar='\"')\n",
    "\n",
    "# take the value in the first column and put it into a list\n",
    "wordlist = df['Unnamed: 0'].values.tolist()\n",
    "\n",
    "# rename the first columns as 'word'\n",
    "df.rename(columns={'Unnamed: 0': 'word'}, inplace=True)\n",
    "\n",
    "# delete the empty columns\n",
    "df.dropna(axis=1, how='all', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all data in first column to be string\n",
    "df['word'] = df['word'].astype(str)\n",
    "\n",
    "# turn the value into lower case\n",
    "df['word'] = df['word'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get the pos_tag for each word\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    # tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag = nltk.pos_tag([word])[0][1]\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    # return tag_dict.get(tag, wordnet.NOUN)\n",
    "    return tag_dict.get(tag[0].upper(), wordnet.NOUN)\n",
    "\n",
    "# define a function to lemmatize the words\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # lemmatized_output = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)])\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in text.split()])\n",
    "    return lemmatized_output\n",
    "\n",
    "# apply the function to the column\n",
    "df['lemmatized'] = df['word'].apply(lemmatize_words)\n",
    "\n",
    "# save the file\n",
    "#df.to_csv('df_lemmatized_0718.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-5224c70ad886>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_lemmatized.drop_duplicates(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# create a new dataframe from the lemmatized column\n",
    "df_lemmatized = df[['lemmatized']]\n",
    "df_lemmatized.drop_duplicates(inplace=True)\n",
    "df_lemmatized.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist_lemmatized = df_lemmatized['lemmatized'].values.tolist()\n",
    "\n",
    "len(wordlist_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuse',\n",
       " 'allegation',\n",
       " 'indict',\n",
       " 'impeach',\n",
       " 'blame',\n",
       " 'not good',\n",
       " 'not fair',\n",
       " 'hold responsible for',\n",
       " 'favoritism']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_forms.word_forms import get_word_forms\n",
    "\n",
    "# get word form the lemmatized column and put it into a list\n",
    "wordlist_lemmatized = df_lemmatized['lemmatized'].values.tolist()\n",
    "\n",
    "def get_all_forms(word):\n",
    "    result =  get_word_forms(word)\n",
    "    all_form = []\n",
    "    for key, value in result.items():\n",
    "        all_form.append(value)\n",
    "\n",
    "    # flatten the list    \n",
    "    all_form = [item for sublist in all_form for item in sublist]\n",
    "\n",
    "    # remove the duplicate\n",
    "    all_form = list(set(all_form))\n",
    "\n",
    "    if all_form == []:\n",
    "        all_form.append(word)\n",
    "    \n",
    "    return all_form\n",
    "\n",
    "# get all forms for each word in the list wordlist_lemmatized\n",
    "whole_list = []\n",
    "\n",
    "for word in wordlist_lemmatized:\n",
    "    all_form = get_all_forms(word)\n",
    "\n",
    "    whole_list.append(all_form)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['accusal',\n",
       "  'accusatory',\n",
       "  'accused',\n",
       "  'accusative_case',\n",
       "  'accusative_cases',\n",
       "  'accusals',\n",
       "  'accusation',\n",
       "  'accusers',\n",
       "  'accusations',\n",
       "  'accusatorial',\n",
       "  'accusing',\n",
       "  'accusive',\n",
       "  'accusative',\n",
       "  'accusatives',\n",
       "  'accuser',\n",
       "  'accuse',\n",
       "  'accuses'],\n",
       " ['allegation', 'allegations'],\n",
       " ['indictments', 'indict', 'indicted', 'indicting', 'indicts', 'indictment'],\n",
       " ['impeached',\n",
       "  'impeachment',\n",
       "  'impeaching',\n",
       "  'impeachments',\n",
       "  'impeach',\n",
       "  'impeaches'],\n",
       " ['blameworthy',\n",
       "  'blameable',\n",
       "  'blamed',\n",
       "  'blameworthinesses',\n",
       "  'blaming',\n",
       "  'blames',\n",
       "  'blameworthiness',\n",
       "  'blame',\n",
       "  'blamable'],\n",
       " ['not good'],\n",
       " ['not fair'],\n",
       " ['hold responsible for'],\n",
       " ['favoritism', 'favoritisms']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the list\n",
    "whole_list = [item for sublist in whole_list for item in sublist]\n",
    "\n",
    "# remove the duplicate\n",
    "whole_list = list(set(whole_list))\n",
    "\n",
    "len(whole_list)\n",
    "\n",
    "# create a dataframe from the list\n",
    "df_whole_list = pd.DataFrame(whole_list, columns=['word'])\n",
    "\n",
    "# sort the dataframe\n",
    "df_whole_list.sort_values(by=['word'], inplace=True)\n",
    "\n",
    "# save the file\n",
    "df_whole_list.to_csv('accusation_add_final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning word form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_form = get_all_forms(wordlist_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_list = []\n",
    "\n",
    "    for word in wordlist:\n",
    "        \n",
    "    \n",
    "    return all_form\n",
    "\n",
    "\n",
    "# get all forms for each word in the list wordlist_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_form"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test word Form, WORKED!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_forms.word_forms import get_word_forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result =  get_word_forms(\"rain\")\n",
    "\n",
    "# take all value from the result dictionary and combine them into a list\n",
    "all_form = []\n",
    "\n",
    "\n",
    "for key, value in result.items():\n",
    "    all_form.append(value)\n",
    "\n",
    "# flatten the list\n",
    "all_form = [item for sublist in all_form for item in sublist]\n",
    "\n",
    "# remove the duplicate\n",
    "all_form = list(set(all_form))\n",
    "\n",
    "all_form"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test different stemming methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def morphify(word,org_pos,target_pos):\n",
    "    \"\"\" morph a word \"\"\"\n",
    "    synsets = wn.synsets(word, pos=org_pos)\n",
    "\n",
    "    # Word not found\n",
    "    if not synsets:\n",
    "        return []\n",
    "\n",
    "    # Get all  lemmas of the word\n",
    "    lemmas = [l for s in synsets \\\n",
    "                   for l in s.lemmas() if s.name().split('.')[1] == org_pos]\n",
    "\n",
    "    # Get related forms\n",
    "    derivationally_related_forms = [(l, l.derivationally_related_forms()) \\\n",
    "                                    for l in    lemmas]\n",
    "\n",
    "    # filter only the targeted pos\n",
    "    related_lemmas = [l for drf in derivationally_related_forms \\\n",
    "                           for l in drf[1] if l.synset().name().split('.')[1] == target_pos]\n",
    "\n",
    "    # Extract the words from the lemmas\n",
    "    words = [l.name() for l in related_lemmas]\n",
    "    len_words = len(words)\n",
    "\n",
    "    # Build the result in the form of a list containing tuples (word, probability)\n",
    "    result = [(w, float(words.count(w))/len_words) for w in set(words)]\n",
    "    result.sort(key=lambda w: -w[1])\n",
    "\n",
    "    # return all the possibilities sorted by probability\n",
    "    return result\n",
    "\n",
    "print(morphify('sadness','n','v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve different tenses and forms of a word\n",
    "# first use lemmentization to get the root word\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# then use wordnet to get the different forms of the word\n",
    "from nltk.corpus import wordnet\n",
    "syns = wordnet.synsets(\"program\")\n",
    "print(syns[0].name())\n",
    "print(syns[0].lemmas()[0].name())\n",
    "#print(syns[0].definition())\n",
    "#print(syns[0].examples())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))\n",
    "\n",
    "# output:\n",
    "# python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do work lemmatization for words in the list\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_word_forms(word):\n",
    "    forms = set()\n",
    "\n",
    "    for synset in wordnet.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            forms.add(lemma.name())\n",
    "\n",
    "    return forms\n",
    "\n",
    "input_word = \"should\"\n",
    "word_forms = get_word_forms(input_word)\n",
    "print(word_forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "#df = pd.read_csv('data/df_0718.csv')\n",
    "\n",
    "# define a function to get the pos_tag for each word\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    # tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag = nltk.pos_tag([word])[0][1]\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    # return tag_dict.get(tag, wordnet.NOUN)\n",
    "    return tag_dict.get(tag[0].upper(), wordnet.NOUN)\n",
    "\n",
    "# define a function to lemmatize the words\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # lemmatized_output = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)])\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in text.split()])\n",
    "    return lemmatized_output\n",
    "\n",
    "# apply the function to the column\n",
    "df['lemmatized'] = df['Unnamed: 0'].apply(lemmatize_words)\n",
    "\n",
    "# save the file\n",
    "df.to_csv('df_lemmatized_0718.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = nltk.pos_tag(['word'])[0][1][0].upper()\n",
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet.synsets('word')[0].lemmas()[0].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(['word'])[0][0].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = nltk.pos_tag(['word'])[0][0]\n",
    "#[0][1]\n",
    "\n",
    "tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict.get(tag.upper(), wordnet.NOUN)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing on wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Get synonyms for a word\n",
    "synonyms = wordnet.synsets(\"happy\")\n",
    "\n",
    "# Get antonyms for a word\n",
    "antonyms = wordnet.synsets(\"happy\")[0].lemmas()[0].antonyms()\n",
    "\n",
    "# Get hypernyms and hyponyms\n",
    "word = wordnet.synsets(\"dog\")[0]\n",
    "hypernyms = word.hypernyms()\n",
    "hyponyms = word.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "word = \"happy\"\n",
    "\n",
    "# Get synsets for the word\n",
    "synsets = wordnet.synsets(word)\n",
    "\n",
    "# Initialize lists to store verb, adjective, and noun forms\n",
    "verbs = []\n",
    "adjectives = []\n",
    "nouns = []\n",
    "\n",
    "for synset in synsets:\n",
    "    for lemma in synset.lemmas():\n",
    "        # Check if the lemma is a verb form\n",
    "        if lemma.synset().pos() == \"v\":\n",
    "            verbs.append(lemma.name())\n",
    "        # Check if the lemma is an adjective form\n",
    "        elif lemma.synset().pos() == \"a\":\n",
    "            adjectives.append(lemma.name())\n",
    "        # Check if the lemma is a noun form\n",
    "        elif lemma.synset().pos() == \"n\":\n",
    "            nouns.append(lemma.name())\n",
    "\n",
    "print(\"Original word:\", word)\n",
    "print(\"Verbs:\", verbs)\n",
    "print(\"Adjectives:\", adjectives)\n",
    "print(\"Nouns:\", nouns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiment spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "word = \"running\"\n",
    "\n",
    "doc = nlp(word)\n",
    "\n",
    "verbs = [token.text for token in doc if token.pos_ == \"VERB\"]\n",
    "nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "\n",
    "print(\"Original word:\", word)\n",
    "print(\"Verbs:\", verbs)\n",
    "print(\"Nouns:\", nouns)\n",
    "print(\"Adjectives:\", adjectives)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try textBolb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "word = \"boxes\"\n",
    "\n",
    "blob = TextBlob(word)\n",
    "\n",
    "singular_form = blob.words[0].singularize()\n",
    "plural_form = blob.words[0].pluralize()\n",
    "\n",
    "print(\"Original word:\", word)\n",
    "print(\"Singular form:\", singular_form)\n",
    "print(\"Plural form:\", plural_form)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "\n",
    "p = inflect.engine()\n",
    "\n",
    "word = \"box\"\n",
    "\n",
    "plural_form = p.plural(word)\n",
    "singular_form = p.singular_noun(word)\n",
    "\n",
    "print(\"Original word:\", word)\n",
    "print(\"Singular form:\", singular_form)\n",
    "print(\"Plural form:\", plural_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"ran\"\n",
    "\n",
    "print(\"Original word:\", word)\n",
    "print(\"Verb tense:\", TextBlob(word).words[0].tags[0][1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.plural_noun(\"I\", N1)\n",
    "p.plural_verb(\"saw\", N1)\n",
    "p.plural_adj(\"my\", N2)\n",
    "p.plural_noun(\"saw\", N2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
